# Transformer(Attention Is All You Need)

[Transformer(Attention Is All You Need).pdf](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Transformer(Attention_Is_All_You_Need).pdf)

Code: [https://wikidocs.net/31379](https://wikidocs.net/31379)

      [https://paul-hyun.github.io/transformer-01/](https://paul-hyun.github.io/transformer-01/)

****[Transformer ì´í•´í•˜ê¸°](https://wikidocs.net/156986)****

[Transformer.py](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Transformer.py)

[ë…¸ì…˜](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Transformer%20Code%201%2008048e05388046aba29cb19296f0c475.md)

---

## 1. ì„œë¡ 

- ê¸°ì¡´ì˜ ëª¨ë¸ì€ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ í¬í•¨í•˜ëŠ” êµ¬ì¡°
- ì¸ì½”ë” ë””ì½”ë” ê¸°ë°˜ìœ¼ë¡œ RNNì´ë‚˜ CNNì„ ì‚¬ìš©

## **2. ê´€ë ¨ ì—°êµ¬**

### **Sequential ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ, ì´ì „ ì—°êµ¬**

Recurrent(ìˆœí™˜) êµ¬ì¡°ëŠ” Inputê³¼ Output Sequenceë¥¼ í™œìš©

$h_t :t-1$ë¥¼  inputê³¼  $h_{t-1}$ ë¥¼ í†µí•´ ìƒì„±

â†’ ì´ëŸ¬í•œ êµ¬ì¡°ë¡œ ì¸í•´ ì¼ê´„ì²˜ë¦¬(ë³‘ë ¬í™”)ê°€ ì œí•œë¨

(âˆµ ìˆœì°¨ì ìœ¼ë¡œÂ tÂ ì´ì „ì˜ Outputì´ ë‹¤ ê³„ì‚°ë˜ì–´ì•¼ ìµœì¢… Outputì´ ìƒì„±)

### **Sequential Computation ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ, ì´ì „ ì—°êµ¬**

- CNN êµ¬ì¡°ë¥¼ í†µí•´ ë³‘ë ¬í™”
    1. ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•œ ì¶”ê°€ ì—°ì‚° í•„ìš”
    2. ì›ê±°ë¦¬ Positionê°„ì˜ Dependencies(ì¢…ì†ì„±)ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
    
    ![Untitled](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Untitled.png)
    
    ê·¸ë¦¼ 1. ConvS25 êµ¬ì¡°. CNNì„ í™œìš©í•œ ë³‘ë ¬í™” ë°©ì•ˆ
    
    ë¹¨ê°„ ë°•ìŠ¤: ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•œ ì¶”ê°€ ì—°ì‚° í•„ìš”
    

---

### Model Architecture

- **[Encoder-Decoder](https://www.notion.so/Seq2seq-d83be7a8cd3043149795fa32930a2289)** êµ¬ì¡°ë¥¼ ê°€ì§

![Untitled](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Untitled%201.png)

ê·¸ë¦¼ 1. íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°

### ì „ì²´ì ì¸ êµ¬ì¡°

![Untitled](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Untitled%202.png)

ê·¸ë¦¼ 2. Encoder-Decoder êµ¬ì¡°

- ì…ë ¥ $(x_1,...,x_n)$ì€ Encoderë¥¼ í†µí•´   $z=(z_1,...,z_n)$ë¡œ í‘œí˜„ ë° ë§¤í•‘ë¨
- Encoderë¡œ í‘œí˜„í•œ zë¥¼ í™œìš©í•˜ì—¬, í•œ Elementì”© Output Sequence $(y_1,...,y_m)$ê°€ ìƒì„±

<aside>
ğŸ’¡ Auto-Regressive: ìƒì„±ëœ Symbolì€ ë‹¤ìŒ ìƒì„± ê³¼ì •ì—ì„œ ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

</aside>

### Encoder êµ¬ì¡°

- N=6ì˜ ë™ì¼í•œ Layer Stackìœ¼ë¡œ êµ¬ì„±
- ê° LayerëŠ” 2ê°œì˜ Sub-Layerë¡œ êµ¬ì„±
    1. Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜
    - 2. Position-wise Fully Connected Feed-Forward Network
        - 1x1 Conv Layerê°€ 2ê°œ ì´ì–´ì§„ ê²ƒê³¼ ê°™ìŒ
        - Positionë³„ë¡œ ë™ì¼í•œ Fully Connected Feed-Forward Networkê°€ ì ìš© (Dim=2048)
- ê° Sub-LayerëŠ” Residual Connection ë° Layer Normalization ì ìš©

$$
LayerNorm(x+SubLayer(x))
$$

- Residual Connection ì ìš©ì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´

Sub-Layer, Embedding, Output Dimensionì„ 512ë¡œ í†µì¼

<aside>
ğŸ’¡ Residual Connection ì ìš©ì„ ìœ„í•´ì„ 
Inputê³¼ ì—°ê²°ëœ Outputì˜ Dimensionì´ ë™ì¼í•´ì•¼ í•¨

</aside>

### Attention ë©”ì»¤ë‹ˆì¦˜

![á„ƒá…¡á„‹á…®á†«á„…á…©á„ƒá…³.png](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/%E1%84%83%E1%85%A1%E1%84%8B%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A9%E1%84%83%E1%85%B3.png)

ê·¸ë¦¼ 2. Attention êµ¬ì¡°

- inputì´ë‚˜ Output Sequenceì— ê´€ê³„ì—†ì´ Dependencies(ì¢…ì†ì„±)ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ

---

## Self-Attention ë©”ì»¤ë‹ˆì¦˜

- ë‹¨ì¼ Sequence ì•ˆì—ì„œ Positionë“¤ì„ ì—°ê²°
- ë…í•´,  ìš”ì•½, Sentence Representatioinì—ì„œ íš¨ê³¼ì 

---

## ìš”ì•½

- ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ë§Œì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ì œì•ˆ

---

ì¶œì²˜:

[[[ë…¼ë¬¸ìš”ì•½] Transformer ë“±ì¥ - Attention Is All You Need(2017) â‘ ]](https://kmhana.tistory.com/28)

[[íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ê°„ë‹¨íˆ ì´í•´í•˜ê¸° (1)]](https://moondol-ai.tistory.com/460)

[Transformer Code 1](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Transformer%20Code%201%2008048e05388046aba29cb19296f0c475.md)

[Transformer Code 2](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Transformer%20Code%202%20c477cfbf5d74460cbde109c32d68b7a0.md)

[Transformer Code 3](Transformer(Attention%20Is%20All%20You%20Need)%2027804034c39b4ccd97be4794d7555088/Transformer%20Code%203%206e02155521c047ffaaedc6bb76d1f3a1.md)